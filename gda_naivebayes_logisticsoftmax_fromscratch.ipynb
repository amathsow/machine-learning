{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from numpy.linalg import inv\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration and Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>urlDrugName</th>\n",
       "      <th>rating</th>\n",
       "      <th>effectiveness</th>\n",
       "      <th>sideEffects</th>\n",
       "      <th>condition</th>\n",
       "      <th>benefitsReview</th>\n",
       "      <th>sideEffectsReview</th>\n",
       "      <th>commentsReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2202</td>\n",
       "      <td>enalapril</td>\n",
       "      <td>4</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>management of congestive heart failure</td>\n",
       "      <td>slowed the progression of left ventricular dys...</td>\n",
       "      <td>cough, hypotension , proteinuria, impotence , ...</td>\n",
       "      <td>monitor blood pressure , weight and asses for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3117</td>\n",
       "      <td>ortho-tri-cyclen</td>\n",
       "      <td>1</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Severe Side Effects</td>\n",
       "      <td>birth prevention</td>\n",
       "      <td>Although this type of birth control has more c...</td>\n",
       "      <td>Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...</td>\n",
       "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1146</td>\n",
       "      <td>ponstel</td>\n",
       "      <td>10</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>No Side Effects</td>\n",
       "      <td>menstrual cramps</td>\n",
       "      <td>I was used to having cramps so badly that they...</td>\n",
       "      <td>Heavier bleeding and clotting than normal.</td>\n",
       "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3947</td>\n",
       "      <td>prilosec</td>\n",
       "      <td>3</td>\n",
       "      <td>Marginally Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>acid reflux</td>\n",
       "      <td>The acid reflux went away for a few months aft...</td>\n",
       "      <td>Constipation, dry mouth and some mild dizzines...</td>\n",
       "      <td>I was given Prilosec prescription at a dose of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>lyrica</td>\n",
       "      <td>2</td>\n",
       "      <td>Marginally Effective</td>\n",
       "      <td>Severe Side Effects</td>\n",
       "      <td>fibromyalgia</td>\n",
       "      <td>I think that the Lyrica was starting to help w...</td>\n",
       "      <td>I felt extremely drugged and dopey.  Could not...</td>\n",
       "      <td>See above</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       urlDrugName  rating         effectiveness  \\\n",
       "0        2202         enalapril       4      Highly Effective   \n",
       "1        3117  ortho-tri-cyclen       1      Highly Effective   \n",
       "2        1146           ponstel      10      Highly Effective   \n",
       "3        3947          prilosec       3  Marginally Effective   \n",
       "4        1951            lyrica       2  Marginally Effective   \n",
       "\n",
       "           sideEffects                               condition  \\\n",
       "0    Mild Side Effects  management of congestive heart failure   \n",
       "1  Severe Side Effects                        birth prevention   \n",
       "2      No Side Effects                        menstrual cramps   \n",
       "3    Mild Side Effects                             acid reflux   \n",
       "4  Severe Side Effects                            fibromyalgia   \n",
       "\n",
       "                                      benefitsReview  \\\n",
       "0  slowed the progression of left ventricular dys...   \n",
       "1  Although this type of birth control has more c...   \n",
       "2  I was used to having cramps so badly that they...   \n",
       "3  The acid reflux went away for a few months aft...   \n",
       "4  I think that the Lyrica was starting to help w...   \n",
       "\n",
       "                                   sideEffectsReview  \\\n",
       "0  cough, hypotension , proteinuria, impotence , ...   \n",
       "1  Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...   \n",
       "2         Heavier bleeding and clotting than normal.   \n",
       "3  Constipation, dry mouth and some mild dizzines...   \n",
       "4  I felt extremely drugged and dopey.  Could not...   \n",
       "\n",
       "                                      commentsReview  \n",
       "0  monitor blood pressure , weight and asses for ...  \n",
       "1  I Hate This Birth Control, I Would Not Suggest...  \n",
       "2  I took 2 pills at the onset of my menstrual cr...  \n",
       "3  I was given Prilosec prescription at a dose of...  \n",
       "4                                          See above  "
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data reading and concatanation\n",
    "data_train=pd.read_table('drugLibTrain_raw.tsv')\n",
    "data_test=pd.read_table('drugLibTest_raw.tsv')\n",
    "data=pd.concat([data_train,data_test])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            enalapril Highly Effective Mild Side Effects\n",
       "1       ortho-tri-cyclen Highly Effective Severe Side ...\n",
       "2                ponstel Highly Effective No Side Effects\n",
       "3         prilosec Marginally Effective Mild Side Effects\n",
       "4         lyrica Marginally Effective Severe Side Effects\n",
       "                              ...                        \n",
       "1031    accutane Considerably Effective Severe Side Ef...\n",
       "1032          proair-hfa Highly Effective No Side Effects\n",
       "1033    accutane Considerably Effective Moderate Side ...\n",
       "1034             divigel Highly Effective No Side Effects\n",
       "1035    claripel-cream Considerably Effective Mild Sid...\n",
       "Length: 4143, dtype: object"
      ]
     },
     "execution_count": 1031,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatanatio of columns in order to get on text column\n",
    "message= data['urlDrugName'] + \" \" + data['effectiveness'] + \" \" + data['sideEffects'] \n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4143,)"
      ]
     },
     "execution_count": 1032,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transofrmation onto a dataframe and NAN values\n",
    "doc= pd.DataFrame(message,columns=['message'])\n",
    "doc['rating']=data['rating']\n",
    "doc.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean data before processing\n",
    "def preprocess_string(str_arg):\n",
    "        cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "        cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "        cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
    "        return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         enalapril highly effective mild side effects\n",
       "1    ortho tri cyclen highly effective severe side ...\n",
       "2             ponstel highly effective no side effects\n",
       "3      prilosec marginally effective mild side effects\n",
       "4      lyrica marginally effective severe side effects\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply process_string function to clean our data\n",
    "doc['message'].head(5).apply(preprocess_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enalapril Highly Effective Mild Side Effects</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ortho-tri-cyclen Highly Effective Severe Side ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ponstel Highly Effective No Side Effects</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prilosec Marginally Effective Mild Side Effects</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lyrica Marginally Effective Severe Side Effects</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  rating\n",
       "0       enalapril Highly Effective Mild Side Effects       4\n",
       "1  ortho-tri-cyclen Highly Effective Severe Side ...       1\n",
       "2           ponstel Highly Effective No Side Effects      10\n",
       "3    prilosec Marginally Effective Mild Side Effects       3\n",
       "4    lyrica Marginally Effective Severe Side Effects       2"
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if there is any null value\n",
    "doc['message'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# tokenizing text column with countVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=preprocess_string).fit(doc['message'])\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "#message column after transformation\n",
    "messages_bow = bow_transformer.transform(doc['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (4143, 27)\n",
      "Amount of Non-Zero occurences:  71665\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 64\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4143, 27)\n"
     ]
    }
   ],
   "source": [
    "#TFIDF application with word frequency representation\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation into a dataframe \n",
    "X1 = pd.DataFrame(messages_tfidf.toarray(), columns=bow_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=doc['message']\n",
    "y=doc['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation function for model selection and validation\n",
    "def k_fold_cross_validation_sets(X, y, k, shuffle=True):\n",
    "    \"\"\" Split the data into k sets of training / test data \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y)\n",
    "\n",
    "    n_samples = len(y)\n",
    "    left_overs = {}\n",
    "    n_left_overs = (n_samples % k)\n",
    "    if n_left_overs != 0:\n",
    "        left_overs[\"X\"] = X[-n_left_overs:]\n",
    "        left_overs[\"y\"] = y[-n_left_overs:]\n",
    "        X = X[:-n_left_overs]\n",
    "        y = y[:-n_left_overs]\n",
    "\n",
    "    X_split = np.split(X, k)\n",
    "    y_split = np.split(y, k)\n",
    "    sets = []\n",
    "    for i in range(k):\n",
    "        X_test, y_test = X_split[i], y_split[i]\n",
    "        X_train = np.concatenate(X_split[:i] + X_split[i + 1:], axis=0)\n",
    "        y_train = np.concatenate(y_split[:i] + y_split[i + 1:], axis=0)\n",
    "        sets.append([X_train, X_test, y_train, y_test])\n",
    "\n",
    "    # Add left over samples to last set as training samples\n",
    "    if n_left_overs != 0:\n",
    "        np.append(sets[-1][0], left_overs[\"X\"], axis=0)\n",
    "        np.append(sets[-1][2], left_overs[\"y\"], axis=0)\n",
    "\n",
    "    return np.array(sets)\n",
    "\n",
    "def shuffle_data(X, y, seed=None):\n",
    "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to divid dataset in X %\n",
    "def divid_dataset(x,y,pourcentage):\n",
    "    x1 = x.sample(frac=pourcentage)\n",
    "    y1 = y.sample(frac=pourcentage)\n",
    "    return x1,y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications: Naive Bayes, Gaussian Discriminant Analysis and Comparison with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_labels):\n",
    "        self.classes = unique_labels #number of class \n",
    "    \n",
    "    # add sentence to bag of words\n",
    "    def addSentToBow(self,example_sentence,bow_dict_index):\n",
    "        ## example_sentance : is the sentence we pass to test/train\n",
    "        ## bow_dict_index : index of each bag of word corresponding to example_sentence\n",
    "        # tokenize every sentence and add it to the dictionnary\n",
    "        \n",
    "        #checking if example sent is of type nd array i.e array([]) and getting the sentence\n",
    "        if(isinstance(example_sentence,np.ndarray)):\n",
    "            example_sentence = example_sentence[0]\n",
    "        for word in example_sentence.split():\n",
    "           ## creating dic {word1: freq1,....,wordn:freqn}\n",
    "            self.bow_label_dict[bow_dict_index][word]+=1\n",
    "            \n",
    "    \n",
    "    def train(self,X,y):\n",
    "        self.data = X\n",
    "        self.labels=y\n",
    "        # Creating dict of n dicts, where n is no.of classes\n",
    "        self.bow_label_dict = np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        # converting data to numpy arrays\n",
    "        if not isinstance(self.data,np.ndarray): \n",
    "            self.data=np.array(self.data)\n",
    "        if not isinstance(self.labels,np.ndarray): \n",
    "            self.labels=np.array(self.labels)\n",
    "        \n",
    "        #Creating bag of words for each Class\n",
    "        #enumerate method will return tuple (counter,element in collection)\n",
    "        for index,label in enumerate(self.classes):\n",
    "            partticular_label = self.data[self.labels==label]\n",
    "            \n",
    "            #apply process function to clean every row of text\n",
    "            cleaned_sent = [preprocess_string(sent) for sent in partticular_label]\n",
    "            cleaned_sent=pd.DataFrame(data=cleaned_sent)\n",
    "            \n",
    "            #now we can construct Bag of words of each particular label\n",
    "            np.apply_along_axis(self.addSentToBow,1,cleaned_sent,index)\n",
    "            \n",
    "            '''\n",
    "            Now we need calculate the terms of our formula:\n",
    "            -> Prior Probability of each class - P(c) -> (no.of sentences belongning to class c)/(total no.of sent)\n",
    "            -> Word Corpus/ Vocabulary |V|\n",
    "            '''   \n",
    "        # Creating an array to store the probability of sentence for each class\n",
    "        proba_label = np.empty(self.classes.shape[0])   \n",
    "        all_words = []\n",
    "        count_of_words_in_each_class = np.empty(self.classes.shape[0])\n",
    "            \n",
    "        for index, label in enumerate(self.classes):\n",
    "            #Prior probability P(c) for each class\n",
    "            proba_label[index] = np.sum(self.labels==label)/float(self.labels.shape[0])\n",
    "            \n",
    "            # getting total counts of all words in each class\n",
    "            count = list(self.bow_label_dict[index].values())\n",
    "            \n",
    "            count_of_words_in_each_class[index] = np.sum(np.array(count))+1\n",
    "            \n",
    "            all_words+=self.bow_label_dict[index].keys()\n",
    "            \n",
    "        # now combining all the words of all the classes to get |V|\n",
    "        self.vocab = np.unique(np.array(all_words))\n",
    "        \n",
    "        self.lenOfVocab = self.vocab.shape[0]\n",
    "        \n",
    "        #Computing denominators of each class i.e (count(c) + |v| + 1) i.e\n",
    "        # count of words of that class + Vocab|v|(adding this to avoide 0 probability \n",
    "        # when word is unknown )+1 :  Laplace Smoothing\n",
    "        denom = np.array([count_of_words_in_each_class[index]+self.lenOfVocab+1 for index,label in enumerate(self.classes)])\n",
    "        \n",
    "        # putting pieces of formula in organized way in this way (bow dict of each class,prior probability,denominator)\n",
    "        self.classes_info = [(self.bow_label_dict[index],proba_label[index],denom[index]) for index,labels in enumerate(self.classes)]\n",
    "        self.classes_info = np.array(self.classes_info)\n",
    "    \n",
    "    \n",
    "    def getPosteriorProba(self,test_sent):\n",
    "        \n",
    "        # To store probabilities of each class\n",
    "        likelihood_prob = np.zeros(self.classes.shape[0])\n",
    "        \n",
    "        # Calculating Probabilities to each label/class\n",
    "        # Formula -> (count of word class c +1)/(count wof words in that class + vocab_len + 1)\n",
    "        for index,labels in enumerate(self.classes):\n",
    "            \n",
    "            for word in test_sent.split():\n",
    "                \n",
    "                # numerator in the above formula\n",
    "                # adding +1 to get rid of zero probability i.e even when the word is not present in our training vocab\n",
    "                # then also probability will not be zero as we are adding 1 it will be 1/something.\n",
    "                word_count = self.classes_info[index][0].get(word,0)+1\n",
    "                word_proba = word_count/float(self.classes_info[index][2])\n",
    "                \n",
    "                # log application of each word to compute easier the probability\n",
    "                likelihood_prob[index]+=np.log(word_proba)\n",
    "                \n",
    "        post_proba = np.empty(self.classes.shape[0])\n",
    "        for index,labels in enumerate(self.classes):\n",
    "            post_proba[index] = likelihood_prob[index]+np.log(self.classes_info[index][1])\n",
    "        \n",
    "        return post_proba;\n",
    "    \n",
    "    \n",
    "    # predict with new sentence text\n",
    "    def predict(self,test_data):\n",
    "        \n",
    "        pred = []\n",
    "        \n",
    "        for sent in test_data:\n",
    "            \n",
    "            cleaned_sent = preprocess_string(sent)\n",
    "            \n",
    "            post_proba = self.getPosteriorProba(cleaned_sent)\n",
    "            \n",
    "            pred.append(self.classes[np.argmax(post_proba)])\n",
    "        return np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_labels,test_labels=train_test_split(X,y,shuffle=True,test_size=0.2,random_state=42,stratify=y)\n",
    "classes=np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = NaiveBayes(classes)\n",
    "nb1.train(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy:  45.59710494571773 %\n"
     ]
    }
   ],
   "source": [
    "proba_sent1=nb1.predict(test_data)\n",
    "#accuracy\n",
    "test_acc=np.sum(proba_sent1==test_labels)/float(len(test_labels)) \n",
    "\n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementating Gaussian Discriminant Analysis\n",
    "class GDA():\n",
    "    def __init__(self):\n",
    "        self.__phi = None\n",
    "        self.__means = None\n",
    "        self.__sigma = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.__classes = np.unique(y)\n",
    "        n_classes = len(self.__classes)\n",
    "        \n",
    "        self.__phi = np.zeros((n_classes, 1))\n",
    "        self.__means = np.zeros((n_classes, n_features))\n",
    "        self.__sigma = 0\n",
    "        for i in range(n_classes):\n",
    "            indexes = np.flatnonzero(y == self.__classes[i])\n",
    "\n",
    "            self.__phi[i] = len(indexes) / n_samples\n",
    "            self.__means[i] = np.mean(X[indexes], axis=0)\n",
    "            self.__sigma += np.cov(X[indexes].shape) * (len(indexes) - 1)\n",
    "            #self.__sigma += np.cov(X[indexes].T) * (len(indexes) - 1)\n",
    "\n",
    "            self.__sigma /= n_samples\n",
    "                    \n",
    "    def calculate_px_py(self,X,u,sigma):\n",
    "        n = X.shape[0]\n",
    "        pi = 3.14\n",
    "        x1=1/((2*np.pi)**(n/2)*np.linalg.det(np.abs(sigma)))\n",
    "        D=np.matrix(X-u)\n",
    "        x2=np.exp(-np.dot(np.matmul(D,np.linalg.inv(sigma)),D))[1]\n",
    "        return x1*x2 \n",
    "    \n",
    "    def compute_Pxyi(self, X, idx):\n",
    "        \"\"\"Probability of X given y\"\"\"\n",
    "        m = X.shape[1]\n",
    "        from scipy.stats import multivariate_normal\n",
    "        sigma_inv = np.linalg.inv(self.sigma[idx])\n",
    "        det_sigma = np.linalg.det(self.sigma[idx])\n",
    "        #mu_i = mu(X, y, idx)\n",
    "        Pxi = (1/((2*np.pi)**(m/2))) \\\n",
    "                *(1/(det_sigma**0.5)) \\\n",
    "                * np.exp(- 0.5*np.sum(((X-self.mu[idx])@sigma_inv)*(X-self.mu[idx]), axis=1))\n",
    "    #     Pxi = np.log(1) \\\n",
    "    #             - np.log((2*np.pi)**(m/2)) \\\n",
    "    #             - np.log(np.sqrt(det_sigma)) \\\n",
    "    #             - np.sum(((X-mu_i)@sigma_inv)*(X-mu_i), axis=1)\n",
    "        return Pxi\n",
    "    \n",
    "    def calculate_py(self,y):\n",
    "        return np.where(y==1,self.__phi,1-self.__phi)                              \n",
    "        \n",
    "    def predict(self, X):\n",
    "        #pdf= calculate_px_py(X,mean,self.__sigma)\n",
    "        pdf = lambda mean: multivariate_normal.pdf(X, mean=mean, cov=self.__sigma)\n",
    "        y_probs = np.apply_along_axis(pdf, 1, self.__means) * self.__phi\n",
    "\n",
    "        return self.__classes[np.argmax(y_probs, axis=0)]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_gda,tst_gda,lbls_gda,ltst_gda=train_test_split(X1,y,shuffle=True,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy:  23.401688781664657 %\n"
     ]
    }
   ],
   "source": [
    "model = GDA()\n",
    "model.fit(tr_gda.values,lbls_gda)\n",
    "predicted = model.predict(tst_gda.values)\n",
    "#accuracy\n",
    "test_acc=np.sum(predicted==ltst_gda)/float(len(ltst_gda)) \n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClass():\n",
    "    def __init__(self, learning_rate=0.01, stopping_criterion=0.01, max_iterations=1000, max_epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.stopping_criterion = stopping_criterion\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_epochs = max_epochs\n",
    "        self.w = None\n",
    "    \n",
    "    def softmax(z):\n",
    "        z -= np.max(z)\n",
    "        sm = (np.exp(z).T / np.sum(np.exp(z))).T\n",
    "        return sm\n",
    "    \n",
    "    def MultinomialRegression_fit(self, x, y):\n",
    "        if(x.shape[0]!=y.shape[0]):\n",
    "            #print (\"Number of examples of features and outputs don't match\")\n",
    "            return\n",
    "        \n",
    "        m = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        k = y.shape[1]\n",
    "        \n",
    "        self.w = np.random.randn(k,n)\n",
    "        gradient = np.zeros((k,n))\n",
    "        cost = np.zeros(self.max_epochs)\n",
    "        \n",
    "        for epoch in range(0,self.max_epochs):\n",
    "            #print (epoch)\n",
    "            for i in range(0,m):\n",
    "                y_hat = softmax(np.reshape(np.matmul(self.w,x[i,:]),k))\n",
    "                for j in range(0,k):\n",
    "                    gradient = (y_hat[j]-y[i,j])*x[i,:]\n",
    "                    self.w[j,:]-=self.learning_rate*gradient\n",
    "                    cost[epoch]-=y[i,j]*np.log(y_hat[j])\n",
    "        \n",
    "#         plt.plot(cost)\n",
    "        return self.w\n",
    "    \n",
    "    def MultinomialRegression_predict(self, x):\n",
    "        m = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        \n",
    "        k = self.w.shape[0]\n",
    "        y = np.zeros((m,k))\n",
    "        \n",
    "        for i in range(0,m):\n",
    "            y[i,:] = softmax(np.reshape(np.matmul(self.w,x[i,:]),k))\n",
    "        \n",
    "            y[y >= 0.5 ] = 1\n",
    "            y[y < 0.5 ] = 0\n",
    "        return  y\n",
    "    \n",
    "    def evaluate(self,test_data,labels):\n",
    "        accuracy = accuracy_score(self.MultinomialRegression_predict(test_data),labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of differents algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.    10 % of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 % of the dataset and split into train and test set\n",
    "data1,label1=divid_dataset(X1,y,0.1)\n",
    "train_data1,test_data1,train_labels1,test_labels1=train_test_split(data1,label1,shuffle=True,test_size=0.2,random_state=42,stratify=label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Naive Bayes:  28.846153846153843 %\n"
     ]
    }
   ],
   "source": [
    "data11,label11=divid_dataset(X,y,0.1)\n",
    "train_data,test_data,train_labels,test_labels=train_test_split(data11,label11,shuffle=True,test_size=0.25,random_state=42,stratify=label11)\n",
    "classes=np.unique(train_labels)\n",
    "model = NaiveBayes(classes)\n",
    "model.train(train_data,train_labels)\n",
    "predicted = model.predict(test_data)\n",
    "#accuracy\n",
    "nb1_test_acc=np.sum(predicted==test_labels)/float(len(test_labels)) \n",
    "print (\"Test Set Accuracy for Naive Bayes: \",nb1_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Gaussian Discriminant Analysis:  22.89156626506024 %\n"
     ]
    }
   ],
   "source": [
    "model = GDA()\n",
    "model.fit(train_data1.values,train_labels1)\n",
    "predicted = model.predict(test_data1)\n",
    "#accuracy\n",
    "gda_test_acc=np.sum(predicted==test_labels1)/float(len(test_labels1)) \n",
    "print (\"Test Set Accuracy for Gaussian Discriminant Analysis: \",gda_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "label12 = (np.arange(len(np.unique(label1))) == label1[:, None]).astype(float)\n",
    "train_data12,test_data12,train_labels12,test_labels12=train_test_split(data1,label12,shuffle=True,test_size=0.25,random_state=42,stratify=label12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Logistic regression:  22.115384615384613 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionClass()\n",
    "classifier.max_epochs=100\n",
    "w_predicted = classifier.MultinomialRegression_fit(train_data12.values,train_labels12)\n",
    "y_hat = classifier.MultinomialRegression_predict(test_data12.values)\n",
    "#accuracy\n",
    "lr_test_acc=classifier.evaluate(test_data12.values,test_labels12)\n",
    "print (\"Test Set Accuracy for Logistic regression: \",lr_test_acc.mean()*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison1: 10% dataset\n",
    "    1. Gaussian Discriminant Analaysis/Logistic Regression\n",
    "    Test Set Accuracy for Gaussian Discriminant Analysis:  22.115384615384613 %\n",
    "    Test Set Accuracy for Logistic regression:  22.115384615384613 %\n",
    " \n",
    "         We get exactly the same result for 10% of the dataset.\n",
    "    \n",
    "    2. Naive Bayes/Logistic Regression\n",
    "      Test Set Accuracy for Naive Bayes:  29.807692307692307 %\n",
    "      Test Set Accuracy for Logistic regression:  22.115384615384613 %\n",
    "      \n",
    "         Naive Bayes perform better for 10 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.    30 % of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 % of the dataset and split into train and test set\n",
    "data2,label2=divid_dataset(X1,y,0.3)\n",
    "train_data2,test_data2,train_labels2,test_labels2=train_test_split(data2,label2,shuffle=True,test_size=0.25,random_state=42,stratify=label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Naive Bayes:  26.04501607717042 %\n"
     ]
    }
   ],
   "source": [
    "data21,label21=divid_dataset(X,y,0.3)\n",
    "train_data21,test_data21,train_labels21,test_labels21=train_test_split(data21,label21,shuffle=True,test_size=0.25,random_state=42,stratify=label21)\n",
    "classes21=np.unique(train_labels21)\n",
    "model = NaiveBayes(classes21)\n",
    "model.train(train_data21,train_labels21)\n",
    "predicted = model.predict(test_data21)\n",
    "#accuracy\n",
    "nb21_test_acc=np.sum(predicted==test_labels21)/float(len(test_labels21)) \n",
    "print (\"Test Set Accuracy for Naive Bayes: \",nb21_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Gaussian Discriminant Analysis:  24.115755627009648 %\n"
     ]
    }
   ],
   "source": [
    "model = GDA()\n",
    "model.fit(train_data2.values,train_labels2)\n",
    "predicted = model.predict(test_data2)\n",
    "#accuracy\n",
    "gda2_test_acc=np.sum(predicted==test_labels2)/float(len(test_labels2)) \n",
    "print (\"Test Set Accuracy for Gaussian Discriminant Analysis: \",gda2_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "label22 = (np.arange(len(np.unique(label2))) == label2[:, None]).astype(float)\n",
    "train_data22,test_data22,train_labels22,test_labels22=train_test_split(data2,label22,shuffle=True,test_size=0.25,random_state=42,stratify=label22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Logistic regression:  24.115755627009648 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionClass()\n",
    "classifier.max_epochs=100\n",
    "w_predicted = classifier.MultinomialRegression_fit(train_data22.values,train_labels22)\n",
    "y_hat = classifier.MultinomialRegression_predict(test_data22.values)\n",
    "#accuracy\n",
    "lr2_test_acc=classifier.evaluate(test_data22.values,test_labels22)\n",
    "print (\"Test Set Accuracy for Logistic regression: \",lr2_test_acc.mean()*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison2: 30 % dataset\n",
    "    1. Gaussian Discriminant Analaysis/Logistic Regression\n",
    "    Test Set Accuracy for Gaussian Discriminant Analysis:  24.115755627009648 %\n",
    "    Test Set Accuracy for Logistic regression:  24.115755627009648 %\n",
    "    \n",
    "     We get almost the same result between GDA and Logistic Regression\n",
    "    \n",
    "    2. Naive Bayes/Logistic Regression\n",
    "      Test Set Accuracy for Naive Bayes:  26.04501607717042 %\n",
    "      Test Set Accuracy for Logistic regression:  24.115755627009648 %\n",
    "      \n",
    "      Naive bayes perform better but his performence decrease when the size of   the dataset increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.      60 % of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 % of the dataset and split into train and test set\n",
    "data3,label3=divid_dataset(X1,y,0.6)\n",
    "train_data3,test_data3,train_labels3,test_labels3=train_test_split(data3,label3,shuffle=True,test_size=0.25,random_state=42,stratify=label3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Naive Bayes:  24.27652733118971 %\n"
     ]
    }
   ],
   "source": [
    "data31,label31=divid_dataset(X,y,0.6)\n",
    "train_data31,test_data31,train_labels31,test_labels31=train_test_split(data31,label31,shuffle=True,test_size=0.25,random_state=42,stratify=label31)\n",
    "classes31=np.unique(train_labels31)\n",
    "model = NaiveBayes(classes31)\n",
    "model.train(train_data31,train_labels31)\n",
    "predicted = model.predict(test_data31)\n",
    "#accuracy\n",
    "nb31_test_acc=np.sum(predicted==test_labels31)/float(len(test_labels31)) \n",
    "print (\"Test Set Accuracy for Naive Bayes: \",nb31_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Gaussian Discriminant Analysis:  24.115755627009648 %\n"
     ]
    }
   ],
   "source": [
    "model = GDA()\n",
    "model.fit(train_data3.values,train_labels3)\n",
    "predicted = model.predict(test_data3)\n",
    "#accuracy\n",
    "gda3_test_acc=np.sum(predicted==test_labels3)/float(len(test_labels3)) \n",
    "print (\"Test Set Accuracy for Gaussian Discriminant Analysis: \",gda3_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "label33 = (np.arange(len(np.unique(label3))) == label3[:, None]).astype(float)\n",
    "train_data33,test_data33,train_labels33,test_labels33=train_test_split(data3,label33,shuffle=True,test_size=0.25,random_state=42,stratify=label33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Logistic regression:  24.115755627009648 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionClass()\n",
    "classifier.max_epochs=100\n",
    "w_predicted = classifier.MultinomialRegression_fit(train_data33.values,train_labels33)\n",
    "y_hat = classifier.MultinomialRegression_predict(test_data33.values)\n",
    "#accuracy\n",
    "lr3_test_acc=classifier.evaluate(test_data33.values,test_labels33)\n",
    "print (\"Test Set Accuracy for Logistic regression: \",lr3_test_acc.mean()*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison3: 60 % dataset\n",
    "    1. Gaussian Discriminant Analaysis/Logistic Regression\n",
    "    Test Set Accuracy for Gaussian Discriminant Analysis:  23.15112540192926 %\n",
    "    Test Set Accuracy for Logistic regression:  24.115755627009648 %\n",
    "    \n",
    "    Logistic Regression obtain legerely better result than GDA\n",
    "    \n",
    "    2. Naive Bayes/Logistic Regression\n",
    "      Test Set Accuracy for Naive Bayes:  24.27652733118971 %\n",
    "      Test Set Accuracy for Logistic regression:  24.115755627009648 %\n",
    "      \n",
    "     Naive Bayes and Logistic regression have almost the same result but the performane of the Naive Bayes decrease on 60 % on the datatset whereas Logistic Regression's score is improving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.      100 % of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 % of the dataset and split into train and test set\n",
    "data4,label4=divid_dataset(X1,y,1)\n",
    "train_data4,test_data4,train_labels4,test_labels4=train_test_split(data4,label4,shuffle=True,test_size=0.25,random_state=42,stratify=label4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Naive Bayes:  21.428571428571427 %\n"
     ]
    }
   ],
   "source": [
    "data41,label41=divid_dataset(X,y,1)\n",
    "train_data41,test_data41,train_labels41,test_labels41=train_test_split(data41,label41,shuffle=True,test_size=0.25,random_state=42,stratify=label41)\n",
    "classes41=np.unique(train_labels41)\n",
    "model = NaiveBayes(classes41)\n",
    "model.train(train_data41,train_labels41)\n",
    "predicted = model.predict(test_data41)\n",
    "#accuracy\n",
    "nb41_test_acc=np.sum(predicted==test_labels41)/float(len(test_labels41)) \n",
    "print (\"Test Set Accuracy for Naive Bayes: \",nb41_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Gaussian Discriminant Analysis:  23.35907335907336 %\n"
     ]
    }
   ],
   "source": [
    "model = GDA()\n",
    "model.fit(train_data4.values,train_labels4)\n",
    "predicted = model.predict(test_data4)\n",
    "#accuracy\n",
    "gda4_test_acc=np.sum(predicted==test_labels4)/float(len(test_labels4)) \n",
    "print (\"Test Set Accuracy for Gaussian Discriminant Analysis: \",gda4_test_acc*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "label44 = (np.arange(len(np.unique(label4))) == label4[:, None]).astype(float)\n",
    "train_data44,test_data44,train_labels44,test_labels44=train_test_split(data4,label44,shuffle=True,test_size=0.25,random_state=42,stratify=label44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy for Logistic regression:  23.35907335907336 %\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegressionClass()\n",
    "classifier.max_epochs=100\n",
    "w_predicted = classifier.MultinomialRegression_fit(train_data44.values,train_labels44)\n",
    "y_hat = classifier.MultinomialRegression_predict(test_data44.values)\n",
    "#accuracy\n",
    "lr4_test_acc=classifier.evaluate(test_data44.values,test_labels44)\n",
    "print (\"Test Set Accuracy for Logistic regression: \",lr4_test_acc.mean()*100,\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison4: 100 % dataset\n",
    "    1. Gaussian Discriminant Analaysis/Logistic Regression\n",
    "    Test Set Accuracy for Gaussian Discriminant Analysis:  23.35907335907336 %\n",
    "    Test Set Accuracy for Logistic regression:  23.35907335907336 %\n",
    "    \n",
    "    when we use all the dataset Logistic Regression and GDA still have almost the same performence.\n",
    "    \n",
    "    2. Naive Bayes/Logistic Regression\n",
    "      Test Set Accuracy for Naive Bayes:  21.428571428571427 %\n",
    "      Test Set Accuracy for Logistic regression:  23.35907335907336 %\n",
    "      \n",
    "      with all the dataset Logistic Regression get better score than Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, Naive Bayes and Gaussiant Discriminant Analysis are all used for classification problems. Indeed the difference is about their leearning mechanism: \n",
    "Naive Bayes and Gaussian Discriminant Analysis are generative model(Bayes Rules and join the distrinution of the features to the target) and Logistic Regression is discriminative model(learning the input to output mapping by minimising the error). \n",
    "Durant our comparison we saw that in general, both logistic regression and discriminant analyses converged in similar results even if we change the size of the dataset.\n",
    "Indeed, When the size of the dataset small, Naive Bayes have better performance than Logistic Regression. So Naive Bayes make supposition that all features are lineary independant. That's why it performe well with small size of data but if the size increase, his performence decrease that's to complexy of the dataset. On the other hand, Logistic regression predict well when the dataset is large enough but if the size of the dataset is small relative to the number of features, including regularisation such as Lasso and Ridge regression can help reduce overfitting and result in a more generalised model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
